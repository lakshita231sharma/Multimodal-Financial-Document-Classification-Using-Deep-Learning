# -*- coding: utf-8 -*-
"""Financial_Document_Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b1YREwc5l6aoGXlo8dq0cmqVA-PGnnGE

### **Required Libraries**
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import spacy
import nltk
import tensorflow as tf
from tensorflow import keras
from bs4 import BeautifulSoup
from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize
from imblearn.combine import SMOTETomek
from wordcloud import WordCloud
from warnings import filterwarnings

filterwarnings('ignore')

# spacy
!python -m spacy download en_core_web_lg

# NLTK
nltk.download('punkt')

"""### **Data Collection**"""

# Dataset download from Kaggle
!kaggle datasets download -d gopiashokan/financial-document-classification-dataset

# Extract the Data from Zip File
!unzip financial-document-classification-dataset.zip

"""### **Data Preprocessing**"""

def text_extract_from_html(html_file):

    # Open and Read the HTML File
    with open(file=html_file, mode='r', encoding='utf-8') as file:
        html_content = file.read()

    # Parse the HTML Content
    soup = BeautifulSoup(html_content, 'html.parser')

    # Extract the Text
    text = soup.get_text()

    # Split the Text and Remove Unwanted Space
    result = [i.strip() for i in text.split()]

    return result

# Extract Text from All HTML Documents

dataset_path = '/content/data'

file_name = []
extracted_text = []
target = []

for folder_name in os.listdir(dataset_path):

    # Make a Target_Folder Path
    path = os.path.join(dataset_path, folder_name)

    for html_file in os.listdir(path):

        # Verify the File as HTML_File or not
        if html_file.endswith('.html'):

            # Make a HTML File Path
            html_file_path = os.path.join(dataset_path, folder_name, html_file)

            # Extract the Text from HTML Document
            result = text_extract_from_html(html_file_path)

            # Append the Data into List
            file_name.append(html_file)
            extracted_text.append(result)
            target.append(folder_name)

# Number of Total Documents
len(file_name), len(extracted_text), len(target)

df = pd.DataFrame({'file_name':file_name, 'text':extracted_text, 'target':target})
df

# spaCy Engine

nlp = spacy.load('en_core_web_lg')

def text_processing(text):

    # Process the Text with spaCy
    doc = nlp(' '.join(text))

    # Tokenization, Lemmatization, and Remove Stopwords, punctuation, digits
    token_list = [
                  token.lemma_.lower().strip()
                  for token in doc
                  if token.text.lower() not in nlp.Defaults.stop_words and token.text.isalpha()
                 ]

    if len(token_list) > 0:
        return ' '.join(token_list)
    else:
        return 'empty'

df['text'] = df['text'].apply(text_processing)

df

# Save the DataFrame into CSV File

df.to_csv('/content/df.csv', index=False)

# Read the CSV File

df = pd.read_csv('/content/df.csv')
df

df['target'].value_counts()

# Text Data Visualization using Word Cloud

def word_cloud(target_category):

    # Filter the Dataframe based on Category
    df1 = df[df['target']==target_category]

    # Join the Text Data into Single Sentence
    text = ' '.join(df1['text'].tolist())

    # Sentence Passing to the WordCloud
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

    return wordcloud

plt.figure(figsize=(16,8))

plt.subplot(1, 2, 1)
img_1 = word_cloud('Balance Sheets')
plt.imshow(img_1, interpolation='bilinear')
plt.axis('off')
plt.title('Balance Sheets')

plt.subplot(1, 2, 2)
img_2 = word_cloud('Cash Flow')
plt.imshow(img_2, interpolation='bilinear')
plt.axis('off')
plt.title('Cash Flow')
plt.show()

plt.figure(figsize=(16,8))

plt.subplot(1, 2, 1)
img_1 = word_cloud('Income Statement')
plt.imshow(img_1, interpolation='bilinear')
plt.axis('off')
plt.title('Income Statement')

plt.subplot(1, 2, 2)
img_2 = word_cloud('Notes')
plt.imshow(img_2, interpolation='bilinear')
plt.axis('off')
plt.title('Notes')
plt.show()

plt.figure(figsize=(8, 4))

img_1 = word_cloud('Others')
plt.imshow(img_1, interpolation='bilinear')
plt.axis('off')
plt.title('Others')
plt.show()

# Tokenize the preprocessed sentences

tokenized_sentence = []

# Iterate the each Sentence from the Dataframe
for sentence in df['text']:

    # Sentence Split into Tokens
    token_list = []
    for token in word_tokenize(sentence):

        # Verify the Duplicates
        if token not in token_list:
            token_list.append(token)

    # Append the Tokens_list and empty it
    tokenized_sentence.append(token_list)
    token_list = []

print(len(tokenized_sentence))
print(tokenized_sentence[0:2])

# Train Word2Vec model

model = Word2Vec(sentences=tokenized_sentence, vector_size=300, window=5, min_count=1)
model

model.wv['profit']

model.wv.most_similar('profit')

# Save the trained model

model.save("word2vec_model.bin")

def sentence_embeddings(sentence, model):

    words = word_tokenize(sentence)                                     # split the sentence into separate words

    vectors = [model.wv[word] for word in words if word in model.wv]    # get the vectors of each words

    if vectors:
        return np.mean(vectors, axis=0)                                 # return the average of vectors

    else:
        return np.zeros(model.vector_size)                              # we set the model parameter in training ---> vector_size = 300

df['embeddings'] = df['text'].apply(lambda x: sentence_embeddings(x, model))
df

# Remove the Text Column

df = df.drop(columns=['file_name', 'text'], axis=1)
df

# List of Unique values in Target Column

target = [i for i in df['target'].unique()]
target.sort(reverse=False)
target

# Encoding the Target Column and Reorder the Columns

df['target'] = df['target'].map({'Balance Sheets':0, 'Cash Flow':1, 'Income Statement':2, 'Notes':3, 'Others':4})
df = df[['embeddings', 'target']]
df.tail()

df['target'].value_counts()

# Handling Imbanlance Dataset

# Convert 'embeddings' column to a 2D array
x = np.array(df['embeddings'].tolist())

y = df['target']

# Apply SMOTETomek
smote_tomek = SMOTETomek(random_state=42)
x_resampled, y_resampled = smote_tomek.fit_resample(x, y)

x.shape, y.shape, x_resampled.shape, y_resampled.shape

# Make a Pandas Dataframe
df1 = pd.DataFrame({'embeddings':x_resampled.tolist(), 'target':y_resampled})
df1

# Verify Imbalance Dataset into Balanced Dataset
df1['target'].value_counts()

# Converting Features into List
features = df1['embeddings'].tolist()
features = np.array(features)
features

# Converting Target into List
target = df1['target'].tolist()
target = np.array(target)
target

# Converting Features and Targets to TensorFlow Tensors
features_tensor = tf.convert_to_tensor(features)
target_tensor = tf.convert_to_tensor(target)

features_tensor

target_tensor

# Creating TensorFlow Dataset from Tensors
dataset = tf.data.Dataset.from_tensor_slices((features_tensor, target_tensor))
dataset

# Split the dataset into batches
batch_size = 32
dataset = dataset.batch(batch_size)
dataset

# Length of Batches in Dataset
len(dataset)

# Maximum Number of data Count
173*32

# No of data in Last Badge
5508-(172*32)

# Define a Function process to Split Dataset into Training(80%), Validation(10%) and Testing(10%) Sets

def train_validation_test_split(dataset, train_size=0.8, validation_size=0.1, test_size=0.1, shuffle=True, shuffle_size=10000):

    dataset_batch_count = len(dataset)                                                  # Batch Images dataset Length is 173
    train_batch_count = int(dataset_batch_count * train_size)                           # int(173*0.8) ---> 138 Batches are Training
    validation_test_batch_count = int(dataset_batch_count * validation_size)            # int(173*0.1) ---> 17 Batches are validation & Remaining Testing

    if shuffle:
        dataset = dataset.shuffle(buffer_size=shuffle_size)                             # Suffling the Dataset help to Model Understand the Data Well

    train_ds = dataset.take(train_batch_count)                                          # First 138 Batches are Training_Dataset
    validation_ds = dataset.skip(train_batch_count).take(validation_test_batch_count)   # Remaining 35 Batches ---> Next 17 Batch are Validation_Dataset
    test_ds = dataset.skip(train_batch_count).skip(validation_test_batch_count)         # Pending 18 Batches are Testing

    return train_ds, validation_ds, test_ds

# Apply the Function in Potato Dataset
train_ds, validation_ds, test_ds = train_validation_test_split(dataset)
len(train_ds), len(validation_ds), len(test_ds)

# Visualize the Text and Label Batches from TensorFlow Dataset

for text_batch, label_batch in train_ds.take(1):
    print(text_batch.numpy()[0].shape)
    print(text_batch.numpy()[0])
    print()

    print(label_batch.numpy().shape)
    print(label_batch.numpy()[0])
    print()

# Overview of Training Dataset Structure and Specifications
train_ds

"""### **Model Building & Training**"""

# Building an Optimized Data Pipeline for Enhanced Performance

# Training Dataset
train_ds = train_ds.cache().shuffle(buffer_size=1000).prefetch(tf.data.AUTOTUNE)

# Validation Dataset
validation_ds = validation_ds.cache().shuffle(buffer_size=1000).prefetch(tf.data.AUTOTUNE)

# Testing Dataset
test_ds = test_ds.cache().shuffle(buffer_size=1000).prefetch(tf.data.AUTOTUNE)

train_ds

# Define the Parameters of Input_shape, Channel and Target

text_features = 300    # Extracted features from text
channel = 1            # Mono Channel
target = 5             # Output Classes

input_shape = (text_features, channel)
input_shape

# Build a RNN Bidirectional LSTM Model

model = keras.Sequential([

            # Input Layer
            keras.layers.InputLayer(input_shape=input_shape),

            # 1st Bidirectional LSTM Layer
            keras.layers.Bidirectional(keras.layers.LSTM(units=64, return_sequences=True)),
            keras.layers.Dropout(0.3),

            # 2nd Bidirectional LSTM Layer
            keras.layers.Bidirectional(keras.layers.LSTM(units=128, return_sequences=True)),
            keras.layers.Dropout(0.3),

            # 3rd Bidirectional LSTM Layer
            keras.layers.Bidirectional(keras.layers.LSTM(units=64)),
            keras.layers.Dropout(0.3),

            # Output Layer
            keras.layers.Dense(units=target, activation='softmax')
        ])

model

# Display the Model Summary
model.summary()

# Compiling the Model with Optimizer, Loss, and Metrics

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
model

# Model Training with Training_Dataset and Evaluated with Validation_Dataset

history = model.fit(train_ds,
                    batch_size=batch_size,
                    epochs=250,
                    verbose=1,
                    validation_data = validation_ds)
history

# Evaluating Model Performance on Test Dataset
model.evaluate(test_ds)

# Accessing Training History Parameters
history.params

# Accessing Keys of Training History
history.history.keys()

# Extracting Accuracy Metrics from Training History
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

# Extracting Loss Metrics from Training History
loss = history.history['loss']
val_loss = history.history['val_loss']

len(acc), len(val_acc), len(loss), len(val_loss)

# Save the Model
model.save('/content/model.h5')

# Load the Model
model = tf.keras.models.load_model('/content/model.h5')

# Visualize the Acuuracy & Loss using Plots

plt.figure(figsize=(12, 3))

plt.subplot(1, 2, 1)
plt.plot(range(len(acc)), acc, label='Training Accuracy')
plt.plot(range(len(val_acc)), val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training vs Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(range(len(loss)), loss, label='Training Loss')
plt.plot(range(len(val_loss)), val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training vs Validation Loss')

"""### **Model Inference**"""

def text_extract_from_html(html_file):

    # Open and Read the HTML File
    with open(file=html_file, mode='r', encoding='utf-8') as file:
        html_content = file.read()

    # Parse the HTML Content
    soup = BeautifulSoup(html_content, 'html.parser')

    # Extract the Text
    text = soup.get_text()

    # Split the Text and Remove Unwanted Space
    result = [i.strip() for i in text.split()]

    return result

def text_processing(text):

    # spaCy Engine
    nlp = spacy.load('en_core_web_lg')

    # Process the Text with spaCy
    doc = nlp(' '.join(text))

    # Tokenization, Lemmatization, and Remove Stopwords, punctuation, digits
    token_list = [
                  token.lemma_.lower().strip()
                  for token in doc
                  if token.text.lower() not in nlp.Defaults.stop_words and token.text.isalpha()
                 ]

    if len(token_list) > 0:
        return ' '.join(token_list)
    else:
        return 'empty'

def sentence_embeddings(sentence):

    words = word_tokenize(sentence)                                     # split the sentence into separate words

    model = Word2Vec.load("word2vec_model.bin")                         # load the trained model

    vectors = [model.wv[word] for word in words if word in model.wv]    # get the vectors of each words

    if vectors:
        return np.mean(vectors, axis=0)                                 # return the average of vectors

    else:
        return np.zeros(model.vector_size)                              # we set the model parameter in training ---> vector_size = 300

def prediction(html_file):

    # Extract the Text from HTML Document
    extracted_text = text_extract_from_html(html_file)

    # Preprocess the Text
    preprocessed_text = text_processing(extracted_text)

    # Text Convert into Embeddings
    features = sentence_embeddings(preprocessed_text)

    # Reshape the features into match the expected input shape of Model
    features = np.expand_dims(features, axis=0)
    features = np.expand_dims(features, axis=2)

    # Convert into Tensors
    features_tensors = tf.convert_to_tensor(features, dtype=tf.float32)

    # Load the Model and Prediction
    model = tf.keras.models.load_model('/content/model.h5')
    prediction = model.predict(features_tensors)

    # Find the Maximum Probability Value
    target_label = np.argmax(prediction)

    # Find the Target_Label Name
    target = {0:'Balance Sheets', 1:'Cash Flow', 2:'Income Statement', 3:'Notes', 4:'Others'}
    predicted_class = target[target_label]

    # Find the Confidence
    confidence = round(np.max(prediction)*100, 2)

    print(f'Predicted Class : {predicted_class}')
    print(f'Confidence : {confidence}%')

html_file = '/content/balance_Sheets_sample.html'
prediction(html_file)

html_file = '/content/cash_flow_sample.html'
prediction(html_file)

html_file = '/content/income_statement_sample.html'
prediction(html_file)

html_file = '/content/notes_sample.html'
prediction(html_file)

html_file = '/content/others_sample.html'
prediction(html_file)